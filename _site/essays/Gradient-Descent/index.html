<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="baidu-site-verification" content="x4U8lzh1X1" /><title>机器学习中的梯度下降法 - 游戏暂停</title><link href="https://fonts.googleapis.com/css?family=Lato:900|Work+Sans" rel="stylesheet"><link rel="stylesheet" href="/css/main.css"><title>机器学习中的梯度下降法 | 游戏暂停</title><meta name="generator" content="Jekyll v3.7.3" /><meta property="og:title" content="机器学习中的梯度下降法" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="李家伟的博客。" /><meta property="og:description" content="李家伟的博客。" /><link rel="canonical" href="http://lijiawei.cc/essays/Gradient-Descent/" /><meta property="og:url" content="http://lijiawei.cc/essays/Gradient-Descent/" /><meta property="og:site_name" content="游戏暂停" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2017-03-22T01:27:00+00:00" /><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@tiewuz" /><script type="application/ld+json"> {"headline":"机器学习中的梯度下降法","dateModified":"2017-03-22T01:27:00+00:00","datePublished":"2017-03-22T01:27:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://lijiawei.cc/essays/Gradient-Descent/"},"description":"李家伟的博客。","@type":"BlogPosting","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://lijiawei.cc/favicon.ico"}},"url":"http://lijiawei.cc/essays/Gradient-Descent/","@context":"http://schema.org"}</script><script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [ ['$','$'] ], displayMath: [ ['$$','$$'] ], processEscapes: true } });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body style="background-color: rgb(255, 255, 255)"><script src="/js/theme.min.js"></script><header> <a href="/"><div class="home"></div></a></header><h1 class="post-headline">机器学习中的梯度下降法</h1><div>2017-03-22</div><ul id="markdown-toc"><li><a href="#原理" id="markdown-toc-原理">原理</a></li><li><a href="#代码实现" id="markdown-toc-代码实现">代码实现</a><ul><li><a href="#生成训练集" id="markdown-toc-生成训练集">生成训练集</a></li><li><a href="#训练模型" id="markdown-toc-训练模型">训练模型</a></li></ul></li></ul><h2 id="原理">原理</h2><p>一个二维线性图像可以这样表示:</p><script type="math/tex; mode=display">y = \theta_{0} + \theta_{1}x</script><p>其中的<script type="math/tex">y</script>表示输出,<script type="math/tex">x</script>表示输入, 我们基于已有<script type="math/tex">m</script>项的训练集</p><script type="math/tex; mode=display">(x_{i} , y_{i} ) \quad i=1,...,m</script><p>训练出最精确的<script type="math/tex">\theta_{0}</script>和<script type="math/tex">\theta_{1}</script>, 这样机器就学习到一个函数<script type="math/tex">h</script>, 因为历史原因我们把它称为假设 (hypothesis), 对于输入为<script type="math/tex">x_{i}</script>的<script type="math/tex">h(x)</script>,<script type="math/tex">\hat y_{i}</script>是其输出, 表示方法如下:</p><script type="math/tex; mode=display">h(x_{i})=\theta_{0} + \theta_{1}x_{i}</script><p>什么叫「最精确」? 每一项<script type="math/tex">\hat y_{i}</script>越接近<script type="math/tex">y_{i}</script>越精确. 也就是说, 对于<script type="math/tex">i=1,...,m</script>中的每一个<script type="math/tex">i</script>,<script type="math/tex">\hat y_{i} - y</script>的和要最小, 为了摆脱正负数的困扰, 我们把每一项平方再求和, 就是<script type="math/tex">(\hat y_{i} - y)^2</script>的和最小, 为了便于之后的求导, 再乘上<script type="math/tex">\frac{1}{2}</script>. 我们把各项误差的平方和称为<script type="math/tex">J(\theta_0, \theta_1)</script>,<script type="math/tex">\sum</script>表示求和, 表示方法如下:</p><script type="math/tex; mode=display">J(\theta_0, \theta_1) = \dfrac {1}{2} \displaystyle \sum _{i=1}^m \left ( \hat{y}_{i}- y_{i} \right)^2 = \dfrac {1}{2} \displaystyle \sum _{i=1}^m \left (h (x_{i}) - y_{i} \right)^2 \quad i=1,2,...,m</script><p>注意, 在这里,<script type="math/tex">x_{i}</script>和<script type="math/tex">y_{i}</script>均为已知量,<script type="math/tex">\theta_0, \theta_1</script>是未知量, 我们要找到<script type="math/tex">\theta_0, \theta_1</script>使得<script type="math/tex">J(\theta_0, \theta_1)</script>最小, 也就是误差最小. 通过求导<script type="math/tex">J(\theta_0, \theta_1)</script>, 我们可以知道<script type="math/tex">J(\theta_0, \theta_1)</script>减小时,<script type="math/tex">\theta_0, \theta_1</script>的方向, 往<script type="math/tex">J(\theta_0, \theta_1)</script>减小的方向踏出一步, 改变<script type="math/tex">\theta_0, \theta_1</script>, 让<script type="math/tex">J(\theta_0, \theta_1)</script>的值下降. 这个要理解有点难度, 我再解释一下. 现在我们把<script type="math/tex">\theta_0</script>忽略, 那么:</p><script type="math/tex; mode=display">h(x_{i})=\theta_{0}x_{i}</script><script type="math/tex; mode=display">J(\theta_0) = \dfrac {1}{2} \sum _{i=1}^m \left (\theta_0 x_{i} - y_{i} \right)^2 \quad i=1,2,...,m</script><p><script type="math/tex">J(\theta_0)</script>很明显是个二次函数, 我就假定这时化简求到的<script type="math/tex">J(\theta_0)</script>是这样:<script type="math/tex">J(\theta_0) = 3\theta_0^{2} + 9</script>横轴是<script type="math/tex">\theta_{0}</script>, 纵轴是<script type="math/tex">J(\theta_{0})</script>, 形状呈抛物线. 用下图表示.</p><p><img src="https://ww3.sinaimg.cn/large/006tNbRwgy1fduzraslutj30ae073glm.jpg" alt="png" /></p><p>假如我们这时候随机取了一点,<script type="math/tex">\theta_{0}=1</script>, 很明显这还不是最低点, 我在这个点上求导得<script type="math/tex">J'(1)</script>, 高中数学告诉我们这个值是<script type="math/tex">\theta_{0} = 1</script>时的切线斜率.</p><p>我们先不管大小, 而是考虑它的正负号, 如果导数为正,<script type="math/tex">\theta_{0}</script>增加,<script type="math/tex">J(\theta_0)</script>增加. 导数为负,<script type="math/tex">\theta_{0}</script>增加,<script type="math/tex">J(\theta_0)</script>减小.</p><p>但我们还可以换个思路, 如果导数为正,<script type="math/tex">J(\theta_0)</script>减小,<script type="math/tex">\theta_{0}</script>减小. 导数为负,<script type="math/tex">J(\theta_0)</script>减小,<script type="math/tex">\theta_{0}</script>增加.</p><p>也就是说, 正号告诉我们若要让<script type="math/tex">J(\theta_0)</script>减小,<script type="math/tex">\theta_{0}</script>应该减小, 往左移动. 负号告诉我们若要让 $J(\theta_0)$ 减小,<script type="math/tex">\theta_{0}</script>应该增大, 往右移动.</p><p><strong>我们可以看到导数的正负性是和<script type="math/tex">\theta_{0}</script>移动方向相反的</strong>, 导数为正就要减<script type="math/tex">\theta_{0}</script>的值, 导数为负就要加<script type="math/tex">\theta_{0}</script>的值. 若要让<script type="math/tex">\theta_{0}</script>更接近最低点, 我们要与导数符号相反的方向更新<script type="math/tex">\theta_{0}</script>的值, 用<script type="math/tex">:=</script>表示我赋予<script type="math/tex">\theta_{0}</script>一个新的值, 也就是<script type="math/tex">\theta_{0} := \theta_{0} - J'(1) \times somevalue</script></p><p>回到<script type="math/tex">J(\theta_0, \theta_1)</script>, 因为<script type="math/tex">J(\theta_0, \theta_1)</script>是个三维图像, 可以想象先在<script type="math/tex">\theta_{0}</script>的方向切一刀取截面, 这就变成一个二维图像来求导,<script type="math/tex">\theta_{1}</script>同理, 这就是偏导数的意义.</p><p>这个求导用<script type="math/tex">\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script>表示, 偏导数用来判断<script type="math/tex">J(\theta_0, \theta_1)</script>减小的方向,<script type="math/tex">\alpha</script>表示一个系数,<script type="math/tex">\alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script>的绝对值就是步长. 踏完之后<script type="math/tex">\theta_0, \theta_1</script>的值就不一样了, 我们获得了新的<script type="math/tex">\theta_0, \theta_1</script>, 用<script type="math/tex">:=</script>表示我赋予<script type="math/tex">\theta_0, \theta_1</script>一个新的值.</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)\quad where \; j=1,2</script><p>现在求<script type="math/tex">\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script>了,<script type="math/tex">j=0</script>时, 沿<script type="math/tex">\theta_1</script>切一刀, 把<script type="math/tex">\theta_1</script>看作常数:</p><script type="math/tex; mode=display">\theta_0 := \theta_0 - \alpha \sum\limits_{i=1}^{m}(\theta_{0}+\theta_{1}x_{i}-y_{i})</script><p><script type="math/tex">j=1</script>同理, 因为<script type="math/tex">\theta_1</script>有系数<script type="math/tex">x_{i}</script>:</p><script type="math/tex; mode=display">\theta_1 := \theta_1- \alpha \sum\limits_{i=1}^{m}((\theta_{0}+\theta_{1}x_{i}-y_{i} )x_{i})</script><p>当<script type="math/tex">\theta_0, \theta_1</script>已经降无可降时, 我们称此时的<script type="math/tex">\theta_0, \theta_1</script>的值收敛, 这时的<script type="math/tex">\theta_0, \theta_1</script>就是我们想要的结果.</p><h2 id="代码实现">代码实现</h2><h3 id="生成训练集">生成训练集</h3><p>生成11个 (x,y), 且 y 加了0~1之间的随机数干扰.</p><script src="https://gist.github.com/iewaij/b4f63c74ba611bfabce77949573a5601.js"></script><p>生成的结果如图, 点是训练集, 直线表示不加干扰时 x 和 y 的关系.</p><p><img src="https://ww2.sinaimg.cn/large/006tNbRwgy1fduzrv1bkkj30ao073aa2.jpg" alt="png" /></p><h3 id="训练模型">训练模型</h3><script type="math/tex; mode=display">\theta_0 := \theta_0 - \alpha \sum\limits_{i=0}^{m}(\theta_{0}+\theta_{1}x_{i}-y_{i} )</script><script type="math/tex; mode=display">\theta_1 := \theta_1- \alpha \sum\limits_{i=0}^{m}((\theta_{0}+\theta_{1}x_{i}-y_{i} )x_{i})= \theta_1- \alpha \sum\limits_{i=0}^{m}(\theta_{0}x_{i}+\theta_{1}x_{i}^2-y_{i}x_{i} )</script><script src="https://gist.github.com/iewaij/e9fa6591b73b04cb92220e76a2fb43e8.js"></script><p>每运算50次, 生成计算机预测的图像, 我们可以看到计算机的预测是如何逼近原图像的.</p><p>这是第一次预测:</p><p><img src="https://ww1.sinaimg.cn/large/006tNbRwgy1fduztoz0g4j30ao073jr9.jpg" alt="" /></p><p>第五十次:</p><p><img src="https://ww2.sinaimg.cn/large/006tNbRwgy1fduzugs4xyj30ao073aa1.jpg" alt="" /></p><p>第一百次:</p><p><img src="https://ww2.sinaimg.cn/large/006tNbRwgy1fduzurjf9xj30ao073weh.jpg" alt="" /></p><p>第一百五十次:</p><p><img src="https://ww3.sinaimg.cn/large/006tNbRwgy1fduzv18stej30ao0733yi.jpg" alt="" /></p><p>第两百次, 基本完成:</p><p><img src="https://ww4.sinaimg.cn/large/006tNbRwgy1fduzvb5ckuj30ao073glk.jpg" alt="" /></p><br><br><div id="disqus_thread"></div><script defer> (function() { var d = document, s = d.createElement('script'); s.src = '//you-xi-zan-ting.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-92264874-1', 'auto'); ga('send', 'pageview');</script></body></html>
